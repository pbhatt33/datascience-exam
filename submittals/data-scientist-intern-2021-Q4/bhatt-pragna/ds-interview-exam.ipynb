{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n</div>\n\n<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">","metadata":{}},{"cell_type":"markdown","source":"---\n# Welcome\n\nWelcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`—please read that first, *before* attempting to answer questions here.\n\n<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n<p style=\"font-weight:bold\">ADVICE</p>\n<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n</div>\n\n<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n<p style=\"font-weight:bold\">WARNING</p>\n\n<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n</div>\n\n\n## Have fun!\n\nRegardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!","metadata":{}},{"cell_type":"markdown","source":"# Exam Questions","metadata":{}},{"cell_type":"markdown","source":"## 1. Algo + Data Structures","metadata":{}},{"cell_type":"markdown","source":"### Q 1.1: Fibionacci\n![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.1.1\nGiven $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n\n$$ F_n = F_{n-1} + F_{n-2}$$\n\nwith initial conditions of:\n\n$$ F_1 = 1,  F_2 = 1$$","metadata":{}},{"cell_type":"code","source":"def fibonacci(n):\n    if n == 1 or n == 2:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"fibonacci(20)","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"6765"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Q 1.1.2\nWhat's the complexity of your implementation?","metadata":{}},{"cell_type":"markdown","source":"Using a recursion tree, we can see that the time complexity of this implementation is O(2^n). ","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.1.3\nConsider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`.","metadata":{}},{"cell_type":"code","source":"def fibonacci2(n):\n    sum_n = [1,1] # Base Cases\n    for i in range(2, n):\n        sum_n.append(sum_n[i-1] + sum_n[i-2])\n    return sum_n[n-1]","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"fibonacci2(20)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"6765"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Q 1.1.4\nWhat's the complexity of your implementation?","metadata":{}},{"cell_type":"markdown","source":"The for loop appends one element to the list sum_n per iteration and it executes n-3 times, therefore the time complexity of this implementation is O(n-3) = O(n). (The other lines in this implementation have time complexity O(1)). ","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.1.5\nWhat are some examples of optimizations that could improve computational performance?\n","metadata":{}},{"cell_type":"markdown","source":"To further optimize computation performance, we could store only the previous two digits instead of storing everything from base case up to to nth Fibonacci number.","metadata":{}},{"cell_type":"markdown","source":"### Q 1.2: Linked List\n![ll img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/500px-Singly-linked-list.svg.png)","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.1\nConsider a [singly linked list](https://en.wikipedia.org/wiki/Linked_list), $L$. Write a function `is_palindrome(L)` that detects if $L$ is a [palindrome](https://en.wikipedia.org/wiki/Palindrome), by returning a bool, `True` or `False`.\n","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.2\nWhat is the complexity of your implementation?","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.3\nConsider an alternative implementation to detect if L is a palindrome and write a new function, `is_palindrome2(L)`.","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.4\nWhat's the complexity of this implementation?\n","metadata":{}},{"cell_type":"markdown","source":"#### Q 1.2.5 \nWhat are some examples of optimizations that could improve computational performance?\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Prob + Stats","metadata":{}},{"cell_type":"markdown","source":"### Q 2.1: Finding $\\pi$ in a random uniform?\n<img src=https://www.epicurus.com/food/recipes/wp-content/uploads/2015/03/Pi-Day.jpg width=\"480\">\n\nGiven a uniform random generator $[0,1)$ (e.g., use your language's standard libary to generate random value), write a a function `compute_pi` to compute [$\\pi$](https://en.wikipedia.org/wiki/Pi).","metadata":{}},{"cell_type":"code","source":"import random\n\ndef compute_pi(n, random_seed = 123):\n    '''\n    Uses uniform random generator to compute pi\n    \n    Parameters:\n    n (int): how many pairs of uniform random values are used in the computation of pi; higher n will increase accuracy of pi\n    random_seed (int): random seed for reproducibility of results\n    \n    Output:\n    float: computed value of pi\n    '''\n    random.seed(random_seed)\n    points = 0\n    for i in range(0,n):\n        x = random.uniform(0, 1)\n        y = random.uniform(0, 1)\n        if (x-0.5)**2 + (y-0.5)**2 <= 0.25:\n            points += 1\n    return (points/n)*4","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"compute_pi(1000000)","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"3.139492"},"metadata":{}}]},{"cell_type":"markdown","source":"### Q 2.2: Making a 6-side die roll a 7?\n\nUsing a single 6-side die, how can you generate a random number between 1 - 7?","metadata":{}},{"cell_type":"markdown","source":"### Q 2.3: Is normality uniform?\n\n<img src=https://rednaxela1618.files.wordpress.com/2014/06/uniformnormal.png width=\"480\">\n\n\nGiven draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?","metadata":{}},{"cell_type":"markdown","source":"### Q 2.4: Should you pay or should you go?\n\n![coin flip](https://lh5.ggpht.com/iwD6MnHeHVAXNBgrO7r4N9MQxxYi6wT9vb0Mqu905zTnNlBciONAA98BqafyjzC06Q=w300)\n\nLet’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you $2^{(n-1)}$ US dollars. How much would you pay me to play this game? Explain.","metadata":{}},{"cell_type":"markdown","source":"### Q 2.5: Uber vs. Lyft\n\n![uber vs lyft](http://usiaffinity.typepad.com/.a/6a01347fc1cb08970c01bb0876bcbe970d-pi)\n\nYou request 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?","metadata":{}},{"cell_type":"markdown","source":"### Q 2.6: Pick your prize\n<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n\nA prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?","metadata":{}},{"cell_type":"markdown","source":"## 3 Conceptual ML","metadata":{}},{"cell_type":"markdown","source":"### Q 3.1 Why study gradient boosting or neural networks?\n\nConsider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n\\begin{equation}\nEPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n\\end{equation}\nwhere EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n\\begin{equation}\nf(x) = \\mathbb{E}\\left[Y|X=x\\right]\n\\end{equation}\n\nThe goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n\nThe simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n\nNow given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n","metadata":{}},{"cell_type":"markdown","source":"### Q 3.2 Model Selection and Assesment\n\nConsider a multiclass classification problem with a large number of features $p >> N$, for e.g $p=10000, N=100$ The task is threefold\n1. Find a \"good\" subset of features that show strong _univariate_ correlation with class labels\n2. Using the \"good\" subset, build a multi class classifier\n3. Estimate the generalization error of the final model\n\nGiven this dataset, outline your approach and please be sure to cover the following\n- Data splitting\n- Model Selection: either estimating the performance of different classifiers or the same classifier with different hyperparameters\n- Model Assessment: having chosen a classifier, estimating the generalization error\n\nAssume all features are numerical, the dataset contains no NULLS, outliers, etc. and doesn't require any preprocessing.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Outlined Steps:\n\n#### Assumptions: Data has no missing values, all features are numerical, no outliers, and doesn't require any preprocessing.\n\n#### Feature Selection:\nDue to the high dimensionality and relatively small sample size of our data, feature selection is important to ensure predictive power and statistical soundness of our models. Since our response variable is categorical and all features are numerical, a good choice for dimensionality reduction is linear discriminant analysis since this will maximize separatability between the classes of the target variable while reducing dimensionality. However, LDA serves more as a feature extraction algorithm and requires variables to be normally distributed with the same variance. If these assumptions are not met or our goal is simply to find a subset of the original features, a better choice here would be the ANOVA F-test (or non-parametric Kruskal Wallis Test) as this will test which features are independent of the target variable. \n\n#### Model Selection:\nAfter a training-test split has been created and dataset is balanced (if not use oversampling techniques), a simple classification method should be used to avoid overfitting especially given the small sample size. A good choice here may be decision trees since they are relativity simple and can be tuned to avoid overfitting easily. Another option here is ensemble methods such as random forest which combine results from multiple classifiers so we are not relying on an individual model.\n\n#### Model Assessment:\nUsing the test set created earlier, one can use classification metrics such as precision/recall, F1 score, confusion matrix, and area under ROC curve to estimate generalization error. If the data was unbalanced, it's important not to rely on accuracy alone and observe model performance on minority class in the test set. ","metadata":{}}]}